train_dataset:
  name: CINEFlowMatchLatentDataset
  args:
    root: /users/PAS2812/mattbendel/CINE_data/latents   # NEW: where precompute wrote latents
    split: train
    num_time_samples: 1               # same sampling semantics as before
    patch_batch: 4

val_dataset:
  name: CINEFlowMatchDataset           # unchanged raw-val path (uses VAE at val)
  args:
    root: /users/PAS2812/mattbendel/CINE_data
    split: val
    normalize: qmag

dataloader:
  train_batch_size: 1     # each item yields a list of clips; we flatten across the batch
  val_batch_size: 1
  num_workers: 8
  pin_memory: false
  shuffle: true

# Frozen VAE (for latent space)
vae:
  import_path: CardiacVAE.model.vae   # python module containing CardiacVAE
  class_name: CardiacVAE
  args:
    in_channels: 2
    z_dim: 16
    dim: 256
    dim_mult: [1, 2] # [1, 2, 4] for 4x spatial downsampling, [1, 2] for 2x spatial downsampling
    num_res_blocks: 4
    attn_scales: []
  load_state_dict_from: '/users/PAS2812/mattbendel/cardiac_vae/videos/step_0195000/state.pt'
  strict_load: false
  resume: false

# FM transformer
model:
  import_path: CardiacFM.model.transformer
  class_name: LatentFlowMatchTransformer
  args:
    latent_channels: 16
    hidden_size: 1392
    depth: 40
    heads: 24
    mlp_ratio: 4.0
    patch_size: [1, 2, 2]   # latent tokens = n'*H'*W'

optim:
  lr: 0.0002
  betas: [0.9, 0.99]
  weight_decay: 0.01
  grad_clip: 1.0
  total_steps: 400000
  ema_decay: 0.999

validation:
  patch_h: 80
  patch_w: 80
  patch_t: 7

sampler:
  num_steps: 50
  variant: bh1
  shift: 3.0

logging:
  project: cardiac-lfm             # NEW
  run_name: wan_fm_videos          # NEW
  log_every_steps: 50
  val_every_steps: 10000
  save_every_steps: 10000
  out_dir: /users/PAS2812/mattbendel/latent_fm
  wandb_dir: /users/PAS2812/mattbendel/wandb   # NEW (optional)
  wandb_cache_dir: /users/PAS2812/mattbendel/wandbcache  # NEW (optional)

